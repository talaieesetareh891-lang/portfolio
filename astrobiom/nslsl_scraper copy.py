from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
import csv
import time
import re
import json
from dataclasses import dataclass
from typing import List, Optional
import socket
from urllib.parse import urljoin, urlparse

@dataclass
class Article:
    title: str
    authors: List[str]
    journal: str
    publication_date: str
    volume: str
    pages: str
    doi: str
    nslsl_id: str
    abstract: str
    publication_type: str
    url: str
    keywords: List[str] = field(default_factory=list)   # ğŸ”¥ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

class NSLSLScraper:
    def __init__(self, headless: bool = False):
        options = webdriver.ChromeOptions()
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø±ÙˆØ±Ú¯Ø±
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--ignore-ssl-errors")
        options.add_argument("--ignore-certificate-errors-spki-list")
        options.add_argument("--disable-web-security")
        options.add_argument("--allow-running-insecure-content")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        if headless:
            options.add_argument("--headless")
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.set_page_load_timeout(60)
        self.driver.implicitly_wait(10)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        self.base_url = "https://extapps.ksc.nasa.gov/NSLSL/"
        print("âœ… Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def search_topic(self, query: str, max_results: int = 20) -> List[Article]:
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø³Ø§ÛŒØª NSLSL Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø´Ø®Øµ"""
        articles = []
        
        try:
            print(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹: {query}")
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ Ø¬Ø³ØªØ¬Ùˆ
            search_url = urljoin(self.base_url, "Search")
            print(f"ğŸŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ Ø¬Ø³ØªØ¬Ùˆ: {search_url}")
            self.driver.get(search_url)
            
            # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡
            WebDriverWait(self.driver, 30).until(
                EC.presence_of_element_located((By.ID, "searchCriteria"))
            )
            print("âœ… ØµÙØ­Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ù„ÙˆØ¯ Ø´Ø¯")
            
            time.sleep(3)
            
            # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ
            search_box = self.driver.find_element(By.ID, "searchCriteria")
            search_box.clear()
            search_box.send_keys(query)
            print(f"âœ… Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ '{query}' ÙˆØ§Ø±Ø¯ Ø´Ø¯")
            
            # Ø§Ù†Ø¬Ø§Ù… Ø¬Ø³ØªØ¬Ùˆ
            search_box.send_keys(Keys.RETURN)
            print("âœ… Ø¬Ø³ØªØ¬Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
            
            # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù†ØªØ§ÛŒØ¬
            time.sleep(10)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ø² ØµÙØ­Ù‡ Ù†ØªØ§ÛŒØ¬
            article_links = self._extract_article_links()
            
            if not article_links:
                print("âŒ Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
                # Ø°Ø®ÛŒØ±Ù‡ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ debug
                with open("debug_results_page.html", "w", encoding="utf-8") as f:
                    f.write(self.driver.page_source)
                print("ğŸ“„ ØµÙØ­Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ debug Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
                return articles
            
            print(f"ğŸ”— {len(article_links)} Ù„ÛŒÙ†Ú© Ù…Ù‚Ø§Ù„Ù‡ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
            
            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ max_results
            article_links = article_links[:max_results]
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡Ø± Ù…Ù‚Ø§Ù„Ù‡
            for i, link in enumerate(article_links, 1):
                print(f"\nğŸ“– Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø§Ù„Ù‡ {i}/{len(article_links)}")
                print(f"ğŸ”— Ù„ÛŒÙ†Ú©: {link}")
                
                article = self._extract_single_article(link, i)
                if article:
                    articles.append(article)
                    print(f"âœ… Ù…Ù‚Ø§Ù„Ù‡ {i} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
                else:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚Ø§Ù„Ù‡ {i}")
                
                # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ ØµÙØ­Ù‡ Ù†ØªØ§ÛŒØ¬
                self.driver.back()
                time.sleep(2)
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø¬Ø³ØªØ¬Ùˆ: {str(e)}")
            
        return articles
    
    def _extract_keywords(self) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ù…Ù‚Ø§Ù„Ù‡"""
        try:
            body_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            # Ø¯Ù†Ø¨Ø§Ù„ Ø¨Ø®Ø´ Keywords Ø¨Ú¯Ø±Ø¯ÛŒÙ…
            match = re.search(r'Keywords\s*(.*?)\s*(?:Publication Types|Languages|Biological Classifications|Attachments|Number of Views)', 
                            body_text, re.DOTALL | re.IGNORECASE)
            if match:
                block = match.group(1).strip()
                keywords = [kw.strip() for kw in block.split("\n") if kw.strip()]
                return keywords
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Keywords: {e}")
        return []

    def _extract_article_links(self) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ø² ØµÙØ­Ù‡ Ù†ØªØ§ÛŒØ¬"""
        links = []
        
        try:
            # Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª
            selectors = [
                "a[href*='/NSLSL/Search/DetailsForId/']",
                "a[href*='DetailsForId']",
                "a[href*='Details']",
                "cite a",
                ".result a",
                "div[class*='result'] a"
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        print(f"âœ… {len(elements)} Ù„ÛŒÙ†Ú© Ø¨Ø§ selector '{selector}' Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
                        for elem in elements:
                            href = elem.get_attribute('href')
                            if href and ('DetailsForId' in href or 'Details' in href):
                                full_url = urljoin(self.base_url, href)
                                if full_url not in links:
                                    links.append(full_url)
                        if links:
                            break
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± selector {selector}: {str(e)}")
                    continue
            
            # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ ØªÙ„Ø§Ø´ Ø¨Ø§ XPath
            if not links:
                xpath_selectors = [
                    "//a[contains(@href, 'DetailsForId')]",
                    "//a[contains(@href, 'Details')]",
                    "//cite//a",
                    "//div[contains(@class, 'result')]//a"
                ]
                
                for xpath in xpath_selectors:
                    try:
                        elements = self.driver.find_elements(By.XPATH, xpath)
                        if elements:
                            print(f"âœ… {len(elements)} Ù„ÛŒÙ†Ú© Ø¨Ø§ XPath '{xpath}' Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
                            for elem in elements:
                                href = elem.get_attribute('href')
                                if href:
                                    full_url = urljoin(self.base_url, href)
                                    if full_url not in links:
                                        links.append(full_url)
                            if links:
                                break
                    except Exception as e:
                        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± XPath {xpath}: {str(e)}")
                        continue
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {str(e)}")
        
        return links
    
    def _extract_single_article(self, article_url: str, article_num: int) -> Optional[Article]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ Ù…Ù‚Ø§Ù„Ù‡
            self.driver.get(article_url)
            time.sleep(5)
            
            # Ø°Ø®ÛŒØ±Ù‡ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ debug
            with open(f"debug_article_{article_num}.html", "w", encoding="utf-8") as f:
                f.write(self.driver.page_source)
            print(f"ğŸ“„ ØµÙØ­Ù‡ Ù…Ù‚Ø§Ù„Ù‡ {article_num} Ø¨Ø±Ø§ÛŒ debug Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø®ØªÙ„Ù
            keywords = self._extract_keywords()
            title = self._extract_title_from_detail_page()
            authors = self._extract_authors_from_detail_page()
            journal = self._extract_journal_from_detail_page()
            publication_date = self._extract_year_from_detail_page()
            volume = self._extract_volume_from_detail_page()
            pages = self._extract_pages_from_detail_page()
            doi = self._extract_doi_from_detail_page()
            nslsl_id = self._extract_nslsl_id_from_detail_page()
            abstract = self._extract_abstract_from_detail_page()
            publication_type = self._extract_publication_type_from_detail_page()
            
            print(f"ğŸ” Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ù„Ù‡ {article_num}:")
            print(f"   Ø¹Ù†ÙˆØ§Ù†: {title[:50]}..." if title else "   Ø¹Ù†ÙˆØ§Ù†: ÛŒØ§ÙØª Ù†Ø´Ø¯")
            print(f"   Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {len(authors)} Ù†ÙØ±")
            print(f"   Ù…Ø¬Ù„Ù‡: {journal[:30]}..." if journal else "   Ù…Ø¬Ù„Ù‡: ÛŒØ§ÙØª Ù†Ø´Ø¯")
            print(f"   ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø±: {publication_date}")
            print(f"   Ú†Ú©ÛŒØ¯Ù‡: {len(abstract)} Ú©Ø§Ø±Ø§Ú©ØªØ±" if abstract else "   Ú†Ú©ÛŒØ¯Ù‡: ÛŒØ§ÙØª Ù†Ø´Ø¯")
            print(f"   NSLSL ID: {nslsl_id}")
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒØ¡ Ù…Ù‚Ø§Ù„Ù‡
            article = Article(
                keywords=keywords,
                title=title,
                authors=authors,
                journal=journal,
                publication_date=publication_date,
                volume=volume,
                pages=pages,
                doi=doi,
                nslsl_id=nslsl_id,
                abstract=abstract,
                publication_type=publication_type,
                url=article_url
            )
            
            # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ø¹Ù†ÙˆØ§Ù† ÛŒØ§ ID Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
            if title or nslsl_id:
                return article
            else:
                print(f"âš ï¸ Ù…Ù‚Ø§Ù„Ù‡ {article_num} Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±Ø¯")
                return None
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚Ø§Ù„Ù‡ {article_num}: {str(e)}")
            return None
    
    def _extract_title_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        # Ø§Ø¨ØªØ¯Ø§ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ Ø§Ø² HTML structure Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒÙ…
        title_selectors = [
            "h1.title",
            "h1",
            "h2.title", 
            "h2",
            ".title h1",
            ".title h2",
            "[class*='title']",
            "#title",
            ".article-title",
            ".publication-title"
        ]
        
        for selector in title_selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                title = element.text.strip()
                if title and len(title) > 10 and not title.startswith(('NASA', 'NSLSL', 'Search')):
                    return title
            except:
                continue
        
        # Ø§Ú¯Ø± Ø§Ø² HTML structure Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² Ù…ØªÙ† ØµÙØ­Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            lines = [line.strip() for line in page_text.split('\n') if line.strip()]
            
            # Ø­Ø°Ù Ø®Ø·ÙˆØ· ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù†
            skip_patterns = [
                'NASA', 'NSLSL', 'Search', 'Home', 'Menu', 'Login',
                'Copyright', 'Privacy', 'Contact', 'Help', 'About',
                'Data provided by', 'Scientific and Technical Information'
            ]
            
            for line in lines:
                if (len(line) > 15 and 
                    not any(pattern in line for pattern in skip_patterns) and
                    not line.startswith(('http', 'www', 'doi:', 'NSLSL ID:', 'Author'))):
                    return line[:200]  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„
        except:
            pass
        
        return ""
    
    def _extract_authors_from_detail_page(self) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        authors = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± HTML structure
        author_selectors = [
            ".authors",
            ".author",
            "[class*='author']",
            ".byline",
            ".creator"
        ]
        
        for selector in author_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and len(text) > 2:
                        # ØªÙ‚Ø³ÛŒÙ… Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†
                        author_list = re.split(r'[,;]|\sand\s|\s&\s', text)
                        for author in author_list:
                            clean_author = re.sub(r'\s+', ' ', author.strip())
                            if len(clean_author) > 2 and clean_author not in authors:
                                authors.append(clean_author)
                if authors:
                    return authors[:10]  # Ø­Ø¯Ø§Ú©Ø«Ø± 10 Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡
            except:
                continue
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…ØªÙ† ØµÙØ­Ù‡
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            author_patterns = [
                r'Author(?:s)?:\s*(.+?)(?:\n|$)',
                r'By:\s*(.+?)(?:\n|$)',
                r'Written by:\s*(.+?)(?:\n|$)',
                r'Creator(?:s)?:\s*(.+?)(?:\n|$)'
            ]
            
            for pattern in author_patterns:
                matches = re.finditer(pattern, page_text, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    author_text = match.group(1).strip()
                    if author_text:
                        author_list = re.split(r'[,;]|\sand\s|\s&\s', author_text)
                        for author in author_list:
                            clean_author = re.sub(r'\s+', ' ', author.strip())
                            if len(clean_author) > 2 and clean_author not in authors:
                                authors.append(clean_author)
                if authors:
                    return authors[:10]
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {str(e)}")
        
        return authors
    
    def _extract_abstract_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú†Ú©ÛŒØ¯Ù‡ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        print("ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ú†Ú©ÛŒØ¯Ù‡...")
        
        # Ø±ÙˆØ´ 1: Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± HTML elements
        abstract_selectors = [
            "#abstract",
            ".abstract",
            ".summary", 
            "[class*='abstract']",
            "[class*='summary']",
            ".description",
            "[id*='abstract']",
            "[id*='summary']",
            "div.abstract",
            "p.abstract",
            "section.abstract"
        ]
        
        for selector in abstract_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and len(text) > 50:
                        print(f"âœ… Ú†Ú©ÛŒØ¯Ù‡ Ø¨Ø§ selector '{selector}' Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                        return text
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± selector {selector}: {str(e)}")
                continue
        
        # Ø±ÙˆØ´ 2: Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ XPath
        xpath_selectors = [
            "//*[contains(@class, 'abstract')]",
            "//*[contains(@id, 'abstract')]",
            "//*[contains(@class, 'summary')]", 
            "//*[contains(text(), 'Abstract')]//following-sibling::*",
            "//*[text()='Abstract']//following-sibling::*",
            "//*[contains(text(), 'Summary')]//following-sibling::*"
        ]
        
        for xpath in xpath_selectors:
            try:
                elements = self.driver.find_elements(By.XPATH, xpath)
                for element in elements:
                    text = element.text.strip()
                    if text and len(text) > 50:
                        print(f"âœ… Ú†Ú©ÛŒØ¯Ù‡ Ø¨Ø§ XPath '{xpath}' Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                        return text
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± XPath {xpath}: {str(e)}")
                continue
        
        # Ø±ÙˆØ´ 3: ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ† Ú©Ù„ ØµÙØ­Ù‡
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            print(f"ğŸ“„ Ø·ÙˆÙ„ Ú©Ù„ Ù…ØªÙ† ØµÙØ­Ù‡: {len(page_text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ú†Ú©ÛŒØ¯Ù‡
            abstract_patterns = [
                r'Abstract\s*[:\-]?\s*(.{50,2000}?)(?:\n\s*\n|\nKeywords?|\nIntroduction|\n[A-Z][A-Z\s]+:|\nReferences|\nConclusion)',
                r'Summary\s*[:\-]?\s*(.{50,2000}?)(?:\n\s*\n|\nKeywords?|\nIntroduction|\n[A-Z][A-Z\s]+:|\nReferences|\nConclusion)',
                r'ABSTRACT\s*[:\-]?\s*(.{50,2000}?)(?:\n\s*\n|\nKEYWORDS?|\nINTRODUCTION|\n[A-Z][A-Z\s]+:|\nREFERENCES|\nCONCLUSION)',
                r'Description\s*[:\-]?\s*(.{50,2000}?)(?:\n\s*\n|\nKeywords?|\nIntroduction|\n[A-Z][A-Z\s]+:|\nReferences)',
                r'Overview\s*[:\-]?\s*(.{50,2000}?)(?:\n\s*\n|\nKeywords?|\nIntroduction|\n[A-Z][A-Z\s]+:|\nReferences)'
            ]
            
            for i, pattern in enumerate(abstract_patterns, 1):
                print(f"ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯Ùˆ {i}: {pattern[:50]}...")
                matches = re.finditer(pattern, page_text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    abstract = match.group(1).strip()
                    abstract = re.sub(r'\s+', ' ', abstract)  # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
                    if len(abstract) > 100:
                        print(f"âœ… Ú†Ú©ÛŒØ¯Ù‡ Ø¨Ø§ Ø§Ù„Ú¯Ùˆ {i} Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {len(abstract)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                        print(f"   Ø´Ø±ÙˆØ¹ Ù…ØªÙ†: {abstract[:100]}...")
                        return abstract
            
            # Ø±ÙˆØ´ 4: Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§
            paragraphs = page_text.split('\n\n')
            print(f"ğŸ“ {len(paragraphs)} Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
            
            for i, para in enumerate(paragraphs):
                para = para.strip()
                if (len(para) > 200 and len(para) < 2000 and 
                    not para.startswith(('NASA', 'NSLSL', 'Copyright', 'Privacy', 'Home', 'Menu', 'Search')) and
                    not any(word in para.upper() for word in ['MENU', 'LOGIN', 'SEARCH', 'NAVIGATION', 'COPYRIGHT'])):
                    
                    # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ø§Øª Ø¹Ù„Ù…ÛŒ Ø¨Ø§Ø´Ø¯
                    scientific_indicators = ['research', 'study', 'analysis', 'method', 'result', 'conclusion', 
                                           'experiment', 'data', 'finding', 'investigation', 'approach']
                    
                    if any(word in para.lower() for word in scientific_indicators):
                        print(f"âœ… Ù…ØªÙ† Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ú†Ú©ÛŒØ¯Ù‡ Ø¯Ø± Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù {i+1} Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {len(para)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                        print(f"   Ø´Ø±ÙˆØ¹ Ù…ØªÙ†: {para[:100]}...")
                        return para[:1500]  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ† ØµÙØ­Ù‡: {str(e)}")
        
        print("âŒ Ú†Ú©ÛŒØ¯Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
        return ""
    
    def _extract_journal_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ù…Ø¬Ù„Ù‡ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            journal_patterns = [
                r'Journal:\s*(.+?)(?:\n|$)',
                r'Published in:\s*(.+?)(?:\n|$)',
                r'Source:\s*(.+?)(?:\n|$)',
                r'Publication:\s*(.+?)(?:\n|$)',
                r'Periodical:\s*(.+?)(?:\n|$)',
                r'In:\s*(.+?)(?:\n|$)'
            ]
            
            for pattern in journal_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    journal = match.group(1).strip()
                    # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø³Ø§Ù„ Ùˆ Ø­Ø¬Ù… Ùˆ ØµÙØ­Ø§Øª
                    journal = re.sub(r'\b\d{4}\b.*', '', journal).strip()
                    journal = re.sub(r'[,;].*', '', journal).strip()
                    if len(journal) > 3:
                        return journal
        except:
            pass
        
        return ""
    
    def _extract_year_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§Ù„ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text

            # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ù„ Ø¯Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
            year_patterns = [
                r'Year:\s*(\d{4})',
                r'Date:\s*.*?(\d{4})',
                r'Published:\s*.*?(\d{4})',
                r'\b(19[5-9]\d|20[0-4]\d)\b',   # Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ 1950-2049
                r'\.\s*(\d{4})\s+vol\.'         # Ù…Ø«Ù„: Plant physiology. 1967 vol. 42:1373-83
            ]

            for pattern in year_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    # Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÛŒâ€ŒØªØ±ÛŒÙ† Ø³Ø§Ù„ (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†)
                    years = [int(year) for year in matches if 1800 <= int(year) <= 2100]
                    if years:
                        return str(max(years))
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§Ù„: {e}")

        return ""
    
    def _extract_volume_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø¬Ù… Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            volume_patterns = [
                r'Volume\s*(\d+)',
                r'Vol\.\s*(\d+)',
                r'V\.\s*(\d+)',
                r'Volume:\s*(\d+)'
            ]
            
            for pattern in volume_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    return match.group(1)
        except:
            pass
        
        return ""
    
    def _extract_pages_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙØ­Ø§Øª Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            page_patterns = [
                r'Pages?\s*(\d+(?:-\d+)?)',
                r'pp?\.\s*(\d+(?:-\d+)?)',
                r'p\.\s*(\d+(?:-\d+)?)',
                r'Pages?:\s*(\d+(?:-\d+)?)'
            ]
            
            for pattern in page_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    return match.group(1)
        except:
            pass
        
        return ""
    
    def _extract_doi_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ DOI Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            doi_patterns = [
                r'doi:\s*(10\.\d+/[^\s]+)',
                r'DOI:\s*(10\.\d+/[^\s]+)',
                r'Digital Object Identifier:\s*(10\.\d+/[^\s]+)'
            ]
            
            for pattern in doi_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    return match.group(1)
        except:
            pass
        
        return ""
    
    def _extract_nslsl_id_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ NSLSL ID Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            nslsl_patterns = [
                r'NSLSL\s*ID:\s*(\d+)',
                r'ID:\s*(\d+)',
                r'Record\s*ID:\s*(\d+)',
                r'Document\s*ID:\s*(\d+)'
            ]
            
            for pattern in nslsl_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    return match.group(1)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² URL
            url_match = re.search(r'/DetailsForId/(\d+)', self.driver.current_url)
            if url_match:
                return url_match.group(1)
                
        except:
            pass
        
        return ""
    
    def _extract_publication_type_from_detail_page(self) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆØ¹ Ø§Ù†ØªØ´Ø§Ø± Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª"""
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            type_patterns = [
                r'Publication Type:\s*(.+?)(?:\n|$)',
                r'Document Type:\s*(.+?)(?:\n|$)',
                r'Type:\s*(.+?)(?:\n|$)'
            ]
            
            for pattern in type_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    return match.group(1).strip()
        except:
            pass
        
        return "Unknown"
    
    def save_to_csv(self, articles: List[Article], filename: str):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª Ø¯Ø± ÙØ§ÛŒÙ„ CSV"""
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            writer.writerow([
                'Title', 'Authors', 'Journal', 'publication_date', 'Volume', 'Pages', 
                'DOI', 'NSLSL_ID', 'Abstract', 'Publication_Type', 'URL', 
            ])
            
            for article in articles:
                writer.writerow([
                    article.title,
                    '; '.join(article.authors),
                    article.journal,
                    article.publication_date,
                    article.volume,
                    article.pages,
                    article.doi,
                    article.nslsl_id,
                    article.abstract[:500] + ('...' if len(article.abstract) > 500 else ''),
                    article.publication_type,
                    article.url
                ])
        
        print(f"âœ… {len(articles)} Ù…Ù‚Ø§Ù„Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def save_to_json(self, articles: List[Article], filename: str):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
        articles_dict = []
        for article in articles:
            articles_dict.append({
                'keywords': article.keywords,
                'title': article.title,
                'authors': article.authors,
                'journal': article.journal,
                'publication_date': article.publication_date,
                'volume': article.volume,
                'pages': article.pages,
                'doi': article.doi,
                'nslsl_id': article.nslsl_id,
                'abstract': article.abstract,
                'publication_type': article.publication_type,
                'url': article.url
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles_dict, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(articles)} Ù…Ù‚Ø§Ù„Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ JSON {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def close(self):
        """Ø¨Ø³ØªÙ† Ù…Ø±ÙˆØ±Ú¯Ø±"""
        try:
            self.driver.quit()
            print("âœ… Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø³ØªÙ‡ Ø´Ø¯")
        except:
            pass

def main():
    print("ğŸš€ NASA NSLSL Advanced Scraper")
    print("=" * 50)
    
    topic = input("ğŸ“ Ù…ÙˆØ¶ÙˆØ¹ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ").strip()
    if not topic:
        topic = "microgravity"
    
    max_results = int(input("ğŸ”¢ Ú†Ù†Ø¯ Ù…Ù‚Ø§Ù„Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ØŸ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 5): ") or "5")
    
    save_format = input("ğŸ’¾ ÙØ±Ù…Øª Ø°Ø®ÛŒØ±Ù‡ (csv/json/both): ").strip().lower()
    if save_format not in ['csv', 'json', 'both']:
        save_format = 'csv'
    
    scraper = None
    try:
        scraper = NSLSLScraper(headless=False)
        articles = scraper.search_topic(topic, max_results)
        
        if articles:
            timestamp = int(time.time())
            
            if save_format in ['csv', 'both']:
                csv_filename = f"{topic}_{timestamp}_articles.csv"
                scraper.save_to_csv(articles, csv_filename)
            
            if save_format in ['json', 'both']:
                json_filename = f"{topic}_{timestamp}_articles.json"
                scraper.save_to_json(articles, json_filename)
            
            print(f"\nğŸ‰ Ú©Ø§Ø± ØªÙ…Ø§Ù… Ø´Ø¯!")
            print(f"ğŸ“Š {len(articles)} Ù…Ù‚Ø§Ù„Ù‡ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
            
            # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
            for i, article in enumerate(articles, 1):
                print(f"\n--- Ù…Ù‚Ø§Ù„Ù‡ {i} ---")
                print(f"Ø¹Ù†ÙˆØ§Ù†: {article.title[:80]}...")
                print(f"Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {'; '.join(article.authors[:2])}")
                print(f"Ù…Ø¬Ù„Ù‡: {article.journal}")
                print(f"ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø±: {article.publication_date}")
                print(f"Ú†Ú©ÛŒØ¯Ù‡: {len(article.abstract)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                print(f"NSLSL ID: {article.nslsl_id}")
                print(f"Ù„ÛŒÙ†Ú©: {article.url}")
                
        else:
            print("âŒ Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
            print("ğŸ’¡ Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ debug Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯")
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {str(e)}")
    
    finally:
        if scraper:
            scraper.close()

if __name__ == "__main__":
    main()
